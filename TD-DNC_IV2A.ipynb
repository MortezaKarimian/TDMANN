{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_g2Ss23Gvrni"
   },
   "outputs": [],
   "source": [
    "basePath='D:\\\\PM2\\\\IV2A_TD-DNC\\\\'\n",
    "dbType='IV2A_Imagery'\n",
    "random_seed = 32654\n",
    "\n",
    "Channels=3\n",
    "features=1000\n",
    "nb_classes=2\n",
    "batch_size=512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "1EODUBQj-zPU"
   },
   "source": [
    "## setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true,
    "id": "Cnx3rIkH6WJc"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.util import deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "hidden": true,
    "id": "VfE76Qxz9l_h",
    "outputId": "947b8072-692f-4ecf-f618-73267cd04ede",
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "import os,sys,inspect,glob,warnings\n",
    "\n",
    "currentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "parentdir = os.path.dirname(currentdir)\n",
    "sys.path.insert(0,parentdir) \n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "sys.path.insert(0,'D:\\\\PM2\\\\')\n",
    "\n",
    "\n",
    "print('**** ok ****')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "id": "IfQXvs0jvxQv",
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "ghjk\n",
    "\n",
    "\n",
    "#!pip uninstall tensorflow==2.2.0 --yes\n",
    "pip uninstall keras --yes\n",
    "\n",
    "      #!pip install argcomplete\n",
    "pip install keras==2.2.4\n",
    "#!pip install tensorflow==1.10\n",
    "\n",
    "\n",
    "\n",
    "pip install ipython-autotime\n",
    "pip install plot_keras_history\n",
    "\n",
    "print('~'*50)\n",
    "\n",
    "\n",
    "%tensorflow_version 1.10\n",
    "import tensorflow\n",
    "print(tensorflow.__version__)\n",
    "print('~'*50)\n",
    "\n",
    "print('ok')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "hidden": true,
    "id": "u1chUhOulg1X",
    "outputId": "fc59a3be-6cae-4e59-fe20-4cbce566eeff",
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "# %tensorflow_version 1.10\n",
    "import tensorflow\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "h11kVYItZ-qn"
   },
   "source": [
    "## Prepare Data 25920"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "btEKncbgaFz0",
    "outputId": "38de573d-7678-4a10-ce59-7971dbccbb71"
   },
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import gc\n",
    "\n",
    "\n",
    "def prepare_data():\n",
    "  #~~~~~~~~~~~~~~~~~~~~~~~~~~~     prepare data   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "  \n",
    "\n",
    "  with open('D:\\ds\\MMCNN_2A_data_raw.pkl','rb') as f: #read\n",
    "    _Data = pkl.load(f)\n",
    "  \n",
    "  print(_Data.shape)\n",
    "  print('-------------------------------------------')\n",
    "\n",
    "\n",
    "  with open('D:\\ds\\MMCNN_2A_label_raw.pkl','rb') as f: #read\n",
    "    _Target = pkl.load(f)\n",
    "  \n",
    "  print(_Target.shape)\n",
    "  print('-------------------------------------------')\n",
    "\n",
    "\n",
    "  _Data = _Data.reshape( _Data.shape[0] , 1000 * 3 )\n",
    "\n",
    "  print(' _Data shape is : ' , _Data.shape)\n",
    "  print(' _Target shape is : ' , _Target.shape)\n",
    "  \n",
    "\n",
    "  return _Data, _Target\n",
    "\n",
    "\n",
    "\n",
    "_Data, _Target= prepare_data()\n",
    "print(' Load data ==> OK  ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "bV9_BW3paQiN",
    "outputId": "fea13d64-d578-4fee-86bc-f49e467767b9"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_and_prepare_Data(_Data, _Target):\n",
    "  X_train, X_test, y_train, y_test = train_test_split( _Data, _Target , test_size=0.3 , random_state=random_seed  , shuffle=True)  #276159\n",
    "\n",
    "\n",
    "\n",
    "  print(\"\\x1b[31m  ....................................  \\x1b[0m\")\n",
    "  print('X_train: ', X_train.shape)\n",
    "  print('X_test: ', X_test.shape)\n",
    "  print('y_train: ', y_train.shape)\n",
    "  print('y_test: ', y_test.shape)\n",
    "  print(\"\\x1b[31m  ................. Total ...................  \\x1b[0m\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  X_train=X_train[:15360]\n",
    "  y_train=y_train[:15360]\n",
    "\n",
    "  X_test=X_test[:7168]\n",
    "  y_test=y_test[:7168]\n",
    "  \n",
    "\n",
    "\n",
    "  print()\n",
    "  print(\"\\x1b[31m  .................. Summary ..................  \\x1b[0m\")\n",
    "  print('X_train: ', X_train.shape)\n",
    "  print('X_test: ', X_test.shape)\n",
    "  print('y_train: ', y_train.shape)\n",
    "  print('y_test: ', y_test.shape)\n",
    " \n",
    "\n",
    "  print(\"\\x1b[31m  ....................................  \\x1b[0m\")\n",
    "\n",
    "\n",
    "\n",
    "  import pickle as pkl\n",
    "  with open(basePath+'Imagery_temp_X_test_IV2A.pkl','wb') as f: #write\n",
    "    pkl.dump( X_test , f)\n",
    "\n",
    "  with open(basePath +'Imagery_temp_y_test_IV2A.pkl','wb') as f: #write\n",
    "    pkl.dump( y_test , f)\n",
    "\n",
    "  del  X_test, y_test\n",
    "\n",
    "\n",
    "  return X_train,y_train\n",
    "\n",
    "\n",
    "X_train, y_train= split_and_prepare_Data(_Data, _Target)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "WH6ViNM0aWdo"
   },
   "outputs": [],
   "source": [
    "del _Data, _Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "2lSAgSfcaZUJ",
    "outputId": "21271d1b-597f-4052-cea2-b8a0b369ec28"
   },
   "outputs": [],
   "source": [
    "print( X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "UZdbvnLQy8Tw"
   },
   "outputs": [],
   "source": [
    "y_train2=[np.where(r==1)[0][0] for r in np.array( y_train ) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "aTb_J9rhuBs6"
   },
   "source": [
    "## Reshaper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true,
    "id": "q-uEtXfXB8XR"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator , TransformerMixin\n",
    "\n",
    "class CustomLSTMReshaper(BaseEstimator , TransformerMixin):\n",
    "    \n",
    "     def fit(self,X,y=None):\n",
    "        return self    \n",
    "    \n",
    "     def transform(self, X, y=None):     \n",
    "        return X.reshape(( len(X), features, Channels ,1 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "JiZUQk7CYqN0"
   },
   "source": [
    "## Train Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "2hBfOO0IYpNL",
    "outputId": "64db53e7-1cae-411a-a35d-8e9209b64ec6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "# https://medium.com/@mostafa.m.ayoub/customize-your-keras-metrics-44ac2e2980bd\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred) #precision_m\n",
    "    recall = recall_m(y_true, y_pred) #recall_m\n",
    "    return 2*( (precision*recall)/(precision+recall+K.epsilon() )   )\n",
    "\n",
    "\n",
    "def specificity_m(y_true, y_pred):\n",
    "    true_negatives = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
    "    possible_negatives = K.sum(K.round(K.clip(1 - y_true, 0, 1)))\n",
    "    return true_negatives / (possible_negatives + K.epsilon())\n",
    "\n",
    "\n",
    "# implement at: https://www.sabinasz.net/unbalanced-classes-machine-learning/\n",
    "def sensitivity_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    return true_positives / (possible_positives + K.epsilon())    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "Fjo7iI5D3Rt8"
   },
   "source": [
    "## Test Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true,
    "id": "i2PvngAx3S3j"
   },
   "outputs": [],
   "source": [
    "\n",
    "## Metric library\n",
    "from imblearn.metrics import sensitivity_score ,specificity_score #geometric_mean_score\n",
    "\n",
    "from sklearn.metrics import(\n",
    "                            accuracy_score,\n",
    "                            precision_score, \n",
    "                            recall_score,\n",
    "                            f1_score,                                                      \n",
    "                            make_scorer,                            \n",
    "                            cohen_kappa_score\n",
    "                            )\n",
    "\n",
    "\n",
    "def make_experiments_for_tuner():\n",
    "       \n",
    "    accuracy = make_scorer(accuracy_score)\n",
    "    precision = make_scorer(precision_score , average = 'macro' )\n",
    "    recall = make_scorer(recall_score , average = 'macro')\n",
    "    f1 = make_scorer(f1_score  , average = 'macro')\n",
    "    sensitivity = make_scorer( sensitivity_score  , average = 'macro')     \n",
    "    specificity = make_scorer( specificity_score , average = 'macro')\n",
    "    kappa= make_scorer(cohen_kappa_score   )\n",
    "\n",
    "    scores_for_grid = {\n",
    "    \"accuracy\":accuracy,\n",
    "    \"precision\":precision, \n",
    "    \"recall\":recall,\n",
    "    \"f1\":f1,\n",
    "    \"sensitivity\":sensitivity,\n",
    "    \"specificity\":specificity,\n",
    "    \"kappa\":kappa\n",
    "    }\n",
    "    \n",
    "    return scores_for_grid\n",
    "\n",
    "\n",
    "def get_experiments(true_label,pred_label):\n",
    "    \n",
    "     return  {\n",
    "    \"test_accuracy\":accuracy_score(true_label,pred_label ),\n",
    "    \"test_precision\":precision_score(true_label,pred_label , average='macro' ), \n",
    "    \"test_recall\":recall_score(true_label,pred_label , average='macro' ),\n",
    "    \"test_f1\":f1_score(true_label,pred_label , average='macro' ),\n",
    "    \"test_specificity\":specificity_score(true_label,pred_label , average='macro'),    \n",
    "    \"test_sensitivity\":sensitivity_score(true_label,pred_label , average='macro'),\n",
    "    \"test_kappa\":cohen_kappa_score(true_label,pred_label   )\n",
    "     }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "0NvX4TCiB3Zs"
   },
   "source": [
    "## model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# need these for ShallowConvNet\n",
    "def square(x):\n",
    "    return K.square(x)\n",
    "\n",
    "def log(x):\n",
    "    return K.log(K.clip(x, min_value = 1e-7, max_value = 10000)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "hidden": true,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def ShallowConvNet(nb_classes, Chans = 64, Samples = 128, dropoutRate = 0.5):\n",
    "    \"\"\" Keras implementation of the Shallow Convolutional Network as described\n",
    "    in Schirrmeister et. al. (2017), Human Brain Mapping.\n",
    "    \n",
    "    Assumes the input is a 2-second EEG signal sampled at 128Hz. Note that in \n",
    "    the original paper, they do temporal convolutions of length 25 for EEG\n",
    "    data sampled at 250Hz. We instead use length 13 since the sampling rate is \n",
    "    roughly half of the 250Hz which the paper used. The pool_size and stride\n",
    "    in later layers is also approximately half of what is used in the paper.\n",
    "    \n",
    "    Note that we use the max_norm constraint on all convolutional layers, as \n",
    "    well as the classification layer. We also change the defaults for the\n",
    "    BatchNormalization layer. We used this based on a personal communication \n",
    "    with the original authors.\n",
    "    \n",
    "                     ours        original paper\n",
    "    pool_size        1, 35       1, 75\n",
    "    strides          1, 7        1, 15\n",
    "    conv filters     1, 13       1, 25    \n",
    "    \n",
    "    Note that this implementation has not been verified by the original \n",
    "    authors. We do note that this implementation reproduces the results in the\n",
    "    original paper with minor deviations. \n",
    "    \"\"\"\n",
    "\n",
    "    # start the model\n",
    "    input_main   = Input((Chans, Samples, 1))\n",
    "    block1       = Conv2D(40, (1, 13), \n",
    "                                 input_shape=(Chans, Samples, 1),\n",
    "                                 kernel_constraint = max_norm(2., axis=(0,1,2)))(input_main)\n",
    "    block1       = Conv2D(40, (Chans, 1), use_bias=False, \n",
    "                          kernel_constraint = max_norm(2., axis=(0,1,2)))(block1)\n",
    "    block1       = BatchNormalization(epsilon=1e-05, momentum=0.1)(block1)\n",
    "    block1       = Activation(square)(block1)\n",
    "    block1       = AveragePooling2D(pool_size=(1, 35), strides=(1, 7))(block1)\n",
    "    block1       = Activation(log)(block1)\n",
    "    block1       = Dropout(dropoutRate)(block1)\n",
    "    flatten      = Flatten()(block1)\n",
    "    dense        = Dense(nb_classes, kernel_constraint = max_norm(0.5))(flatten)\n",
    "    softmax      = Activation('softmax')(dense)\n",
    "    \n",
    "    return Model(inputs=input_main, outputs=softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### DNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "6Prm8UPC9Q4k"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense,Flatten,  Conv2D,BatchNormalization, AveragePooling2D, Dropout\n",
    "from tensorflow.keras.layers import SpatialDropout2D, Reshape\n",
    "from tensorflow.keras.optimizers import  Adamax\n",
    "from tensorflow.keras.regularizers import l2, l1, l1_l2\n",
    "from tensorflow.keras.constraints import max_norm\n",
    "from tensorflow.keras.layers import Activation\n",
    "\n",
    "\n",
    "def gen_model_DNC( ): \n",
    "  \n",
    "    inp = tf.keras.Input(shape=(features,Channels,1))\n",
    "    c1 =Conv2D(30, (13, 1),kernel_regularizer=l2(0.001), kernel_constraint = max_norm(2., axis=(0,1,2)))(inp)\n",
    "    c2=Conv2D(30, (1, Channels ), use_bias=False, kernel_regularizer=l1_l2(l1=1e-5, l2=0.001), kernel_constraint = max_norm(2., axis=(0,1,2)))(c1)\n",
    "    bn=BatchNormalization(epsilon=1e-05, momentum=0.1)(c2)\n",
    "    ac1=Activation(square)(bn)\n",
    "    avg1= AveragePooling2D(pool_size=(100,1) , strides=(100, 1)  )(ac1)\n",
    "    ac2= Activation(log)(avg1)\n",
    "    sdout= SpatialDropout2D( 0.15 )(ac2)\n",
    "\n",
    "    shallowConvNet_dim1=9\n",
    "    shallowConvNet_dim2=30\n",
    "    rs=Reshape(( shallowConvNet_dim1 , shallowConvNet_dim2 ))(sdout)\n",
    "\n",
    "#     ----------------------------------  \n",
    "    x = tf.keras.Input(shape=(None, 30, ))\n",
    "    dnc_cell = DNC( \n",
    "      30,\n",
    "      controller_units=64,\n",
    "      memory_size=18,\n",
    "      word_size=8,\n",
    "      num_read_heads=2\n",
    "    )\n",
    "    dnc_initial_state = dnc_cell.get_initial_state(batch_size=512)\n",
    "    rnn = tf.keras.layers.RNN(dnc_cell, return_sequences=True)\n",
    "    rnn._name ='DNC_layer'\n",
    "    y = rnn( rs, initial_state=dnc_initial_state)\n",
    "#     ----------------------------------\n",
    "\n",
    "\n",
    "    fl=Flatten ()(y)\n",
    "    fc1 = Dense( 16 )(fl)\n",
    "    do = Dropout(0.3 )(fc1)\n",
    "    fc2 = Dense(2,activation='sigmoid')(do)\n",
    "    model55 = tf.keras.models.Model(inputs=inp, outputs= fc2)\n",
    "    \n",
    "    \n",
    "\n",
    "    opt=Adamax(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=1e-5)     \n",
    "    model55.compile(loss='binary_crossentropy', optimizer=opt,  metrics=['binary_accuracy'  , precision_m, recall_m, f1_m, sensitivity_m, specificity_m  ])\n",
    "    return model55\n",
    "\n",
    "b = gen_model_DNC()\n",
    "b.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true,
    "id": "igSfgMLqu2Or"
   },
   "source": [
    "### Model_Maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true,
    "id": "OMSSjhNwu005"
   },
   "outputs": [],
   "source": [
    "def Model_Maker( ):  \n",
    "  \n",
    "\n",
    "  return gen_model_DNC() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "he6ydZETuYlR"
   },
   "source": [
    "## Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true,
    "id": "5OnFdmyIuXRr"
   },
   "outputs": [],
   "source": [
    "## Pipeline library\n",
    "from imblearn.pipeline import Pipeline, make_pipeline\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "   \n",
    "def make_pipeline():    \n",
    "\n",
    "    main_pipeline = Pipeline(steps =[\n",
    "        ('scaler',StandardScaler() ),           \n",
    "        ('Reshaping' , CustomLSTMReshaper()),         \n",
    "        ('classifier', KerasClassifier(  build_fn=Model_Maker  , batch_size=512, verbose=1, epochs=700    ))       \n",
    "    ],  verbose=True )\n",
    "    \n",
    "    return main_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "SE44B-tSXD9d"
   },
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true,
    "id": "YMkgKQnVXHFA"
   },
   "outputs": [],
   "source": [
    "from plot_keras_history import plot_history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def Visualize_Result(mp , k, path ,  dbType): \n",
    "\n",
    "  #pip install plot_keras_history\n",
    "  #https://pypi.org/project/plot-keras-history/\n",
    "\n",
    "  estimator= mp['classifier'] \n",
    "  history=estimator.model.history.history\n",
    "\n",
    "\n",
    "  # plot_history(history)\n",
    "  # plt.show()\n",
    "\n",
    "  plot_history(history, path= path + dbType +\"_\"+ \"_fold(\"+ str(k) +\").png\" )\n",
    "  plt.close()\n",
    "  return \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "J-hSY5oGoC2s"
   },
   "source": [
    "# post_Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true,
    "id": "WM3KGj50Z2Ud"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from matplotlib import pyplot as pl\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "def dict_mean(dict_list):\n",
    "  mean_dict = {}\n",
    "  std_dict = {}\n",
    "  \n",
    "\n",
    "  for key in dict_list[0].keys():\n",
    "      mean_dict[key] = np.mean([d[key] for d in dict_list], axis=0)\n",
    "      std_dict[key] = np.std([d[key] for d in dict_list], axis=0)\n",
    "  \n",
    "  return mean_dict, std_dict\n",
    "\n",
    "\n",
    "j=0 \n",
    "\n",
    "def Auto_All_History_plot(all_history, savepath , dbType):\n",
    "  #print(  all_history  )\n",
    "  result=dict_mean(all_history )\n",
    "  \n",
    "  print()\n",
    "  for val_metric_name in result[0]:\n",
    "    #print('key', val_metric_name )\n",
    "    \n",
    "    if str( val_metric_name[0:4]) !='val_': # This is a condition for not calculating the results for both train and validation categories\n",
    "        #break                               # Calculations are performed for train category and evaluation is also considered for validation\n",
    "        continue\n",
    "        \n",
    "         \n",
    "    metric_name=str( val_metric_name[4:])\n",
    "\n",
    "\n",
    "    mean=result[0][ metric_name ] # mean\n",
    "    std=result[1][ metric_name ] #std\n",
    "\n",
    "\n",
    "    val_mean=result[0][ val_metric_name ] # mean\n",
    "    val_std=result[1][ val_metric_name ] #std\n",
    "\n",
    "\n",
    "    pl.plot(mean ,color='blue' , marker=\"x\", markersize=10 ,markevery=70  )  \n",
    "    pl.plot(val_mean ,color='red'  ,linestyle='dashed', markersize=20 )\n",
    "\n",
    "    pl.fill_between(  range(len(mean)) , mean-std, mean + std , alpha=0.3, label='Train' ,color='blue' )\n",
    "    pl.fill_between(  range(len(val_mean)) , val_mean-val_std, val_mean + val_std , alpha=0.3 , label='Validation' ,color='red' )\n",
    "\n",
    "    if str( metric_name[-2:] )=='_m':\n",
    "        metric_name=metric_name[:-2]\n",
    "    \n",
    "\n",
    "    plt.title( 'The Model '+metric_name+' Plot')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel( metric_name )\n",
    "\n",
    "    plt.legend(loc='best')\n",
    "    #pl.show()\n",
    "    plt.savefig( savepath+dbType+ '_'+ metric_name+'.png')\n",
    "    plt.close()\n",
    "\n",
    "  return\n",
    "Auto_All_History_plot( all_history , basePath ,dbType)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "fz35La2wB_E_"
   },
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true,
    "id": "h9__qqxhVXWx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 123\n"
     ]
    }
   ],
   "source": [
    "# test Unseen data\n",
    "import pickle as pkl\n",
    "\n",
    "def LoadUnseenData():\n",
    "  \n",
    "  with open(basePath+'Imagery_temp_X_test_IV2A.pkl','rb') as f: #read\n",
    "    X_test = pkl.load(f)\n",
    "    \n",
    "  #print(X_test.shape)\n",
    "  \n",
    "\n",
    "  with open(basePath+'Imagery_temp_y_test_IV2A.pkl','rb') as f: #read\n",
    "    Y_test = pkl.load(f)\n",
    "    \n",
    "    \n",
    "    \n",
    "    Y_test=[np.where(r==1)[0][0] for r in Y_test]\n",
    "    \n",
    "  #print(Y_test.shape)\n",
    "  print('-------------------------------------------')\n",
    "\n",
    "  return X_test, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "U-m8GMo1CToC"
   },
   "source": [
    "# over_all_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true,
    "id": "Elooi83OJjD7"
   },
   "outputs": [],
   "source": [
    "def over_all_metrics_stat(skf_metrics, savePath):\n",
    "  # Take mean and std over all folds \n",
    "  average_metrics_all_folds = dict(pd.DataFrame(skf_metrics).mean())\n",
    "  std_metrics_all_folds = dict(pd.DataFrame(skf_metrics).std())\n",
    "\n",
    "  \n",
    "\n",
    "  average_metrics_all_folds = {str( k[5:])+\"_avg_folds\":v for k,v in average_metrics_all_folds.items()}\n",
    "  std_metrics_all_folds = {str( k[5:])+\"_std_folds\":v for k,v in std_metrics_all_folds.items()}\n",
    "\n",
    "  print('_________ Training metric for all k-fold _________')\n",
    "  print('average_metrics_all_folds')\n",
    "  print(average_metrics_all_folds)\n",
    "  print()\n",
    "  print('std_metrics_all_folds')    \n",
    "  print(std_metrics_all_folds)\n",
    "  print()\n",
    "\n",
    "\n",
    "  df_average_metrics_all_folds = pd.DataFrame([average_metrics_all_folds])\n",
    "  df_std_metrics_all_folds = pd.DataFrame([std_metrics_all_folds])\n",
    "\n",
    "  pd.concat([df_average_metrics_all_folds, df_std_metrics_all_folds ],axis=1 ).to_csv( savePath + dbType + '_UnSeen_data_metric_avg_std.csv')\n",
    "\n",
    "\n",
    "  return     \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "qdRHc9ewBXdn"
   },
   "source": [
    "# k-fold Writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "_8GcYyOfmQzF",
    "outputId": "6655fe2f-7199-433c-ce4c-da0eecaa1258"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "n_splits=5\n",
    "skf = StratifiedKFold(n_splits=n_splits, shuffle=False ) #,random_state=random_seed\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "folds = {}\n",
    "count = 1\n",
    "for train_index, val_index in skf.split(X_train, y_train2 ):\n",
    "    folds['fold_{}'.format(count)] = {}\n",
    "    folds['fold_{}'.format(count)]['train_index'] = train_index.tolist()\n",
    "    folds['fold_{}'.format(count)]['val_index'] = val_index.tolist()\n",
    "    count += 1\n",
    "print(len(folds) == n_splits)#assert we have the same number of splits\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "id": "yCLhu9U9o042"
   },
   "outputs": [],
   "source": [
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ RUN ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "import pandas as pd\n",
    "import gc\n",
    "from keras.utils.np_utils import to_categorical \n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "i=1\n",
    " \n",
    "for key, val in folds.items():\n",
    "\n",
    "\n",
    "  print(key)\n",
    "  print(i)\n",
    "\n",
    "  train_index = val['train_index']\n",
    "  val_index = val['val_index']\n",
    "\n",
    "\n",
    "  \n",
    "  skf_X_train, skf_X_val = (pd.DataFrame( X_train )).iloc[train_index], (pd.DataFrame( X_train)).iloc[val_index]\n",
    "  skf_y_train, skf_y_val = (pd.DataFrame( y_train2 )).iloc[train_index], (pd.DataFrame( y_train2 )).iloc[val_index]\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "  with open( basePath + 'kfold_data_variable ('+ str(i) +').pkl', 'wb') as p: \n",
    "      pickle.dump([ skf_X_train, skf_X_val, skf_y_train, skf_y_val  ], p)\n",
    "\n",
    "\n",
    "  print('_step_',i)\n",
    "  i+=1\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "id": "zl31eNvfBbD1"
   },
   "source": [
    "# k-fold Reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "hidden": true,
    "id": "xu7j-71qH7E3",
    "outputId": "23af0b44-5f21-4665-fd4d-4328c53fc42c"
   },
   "outputs": [],
   "source": [
    "# load previous config\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "try:\n",
    "  with open(basePath + dbType +'_Kfold_Imagery_all_history.pkl','rb') as f: #read\n",
    "    all_history = pkl.load(f)\n",
    "\n",
    "  with open(basePath + dbType+ '_UnSeen_data_metric_avg_std.pkl','rb') as f: #read\n",
    "    skf_metrics = pkl.load(f)\n",
    "\n",
    "\n",
    "except:\n",
    "  print('var empty mod')\n",
    "  all_history= []\n",
    "  skf_metrics= []\n",
    " \n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNOpWwyZD0xF"
   },
   "source": [
    "# Main Excutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oZYGO1J0yKhG",
    "outputId": "fb7cd500-ccf6-4fae-f445-b1aa0d943b34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(all_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JI9dq7aTqM1m",
    "outputId": "f0dcd4c5-8386-4375-e7a9-e1a219540eb4",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ RUN ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "import pandas as pd\n",
    "import gc\n",
    "from keras.utils.np_utils import to_categorical \n",
    "import matplotlib.pyplot as plt\n",
    "import pickle \n",
    "\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "i=5\n",
    "\n",
    "\n",
    "with open( basePath + 'kfold_data_variable ('+ str( i) +').pkl' ,'rb') as p:   \n",
    "    skf_X_train, skf_X_val, skf_y_train, skf_y_val  = pickle.load(p)\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "print(\"============ Training on fold ({}) ==============\".format(i))\n",
    "\n",
    "#skf_y_val2=[np.where(r==1)[0][0] for r in np.array(skf_y_val) ]\n",
    "\n",
    "\n",
    "skf_y_train2= to_categorical( skf_y_train , num_classes=2)\n",
    "skf_y_val2= to_categorical( skf_y_val , num_classes=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pipeline = make_pipeline( ) \n",
    "\n",
    "# ------------------------\n",
    "pipeline.named_steps['scaler'].fit(skf_X_train)\n",
    "skf_X_val22=pipeline.named_steps['scaler'].transform(skf_X_val)\n",
    "skf_X_val2=pipeline.named_steps['Reshaping'].transform(skf_X_val22)\n",
    "# ------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pipeline.fit(skf_X_train, skf_y_train2 , classifier__validation_data=( skf_X_val2, skf_y_val2)  )\n",
    "\n",
    "gc.collect()\n",
    "del skf_X_train, skf_y_train, skf_y_val, skf_y_train2, skf_y_val2, skf_X_val2, skf_X_val22, skf_X_val\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "X_test, Y_test =LoadUnseenData()\n",
    "\n",
    "y_pred_val = pipeline.predict( X_test  )\n",
    "del X_test\n",
    "gc.collect()\n",
    "\n",
    "print(\"_________________________________________________\")\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print('___step___',i)\n",
    "print()\n",
    "print('Model Evaluation is in process...')\n",
    "test_results = get_experiments( Y_test , y_pred_val)\n",
    "print(test_results)\n",
    "del Y_test , y_pred_val\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "skf_metrics.append(test_results)\n",
    "\n",
    "\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "all_history.append( pipeline['classifier'].model.history.history )\n",
    "print('Model Training history has been store')\n",
    "print()\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "\n",
    "Visualize_Result(pipeline , i, basePath ,   dbType )\n",
    "print('Evaluation Resault has been Visualized')\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "\n",
    "# # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "del pipeline\n",
    "gc.collect()\n",
    "\n",
    "# # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "\n",
    "with open(basePath + dbType +'_Kfold_Imagery_all_history.pkl','wb') as f: #write\n",
    "  pkl.dump( all_history , f)\n",
    "print('Model Training history Resault has been wrote on file')\n",
    "\n",
    "\n",
    "with open(basePath + dbType +'_UnSeen_data_metric_avg_std.pkl','wb') as f: #write\n",
    "  pkl.dump( skf_metrics , f)\n",
    "print('Model Evaliuation Resault has been wrote on file')  \n",
    "\n",
    "\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_history))\n",
    "#all_history.append( pipeline['classifier'].model.history.history )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9-oTLcvLGYBH"
   },
   "outputs": [],
   "source": [
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# get results\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "# pd.DataFrame([skf_metrics]).to_csv( basePath+ dbType + '_All_fold_Results_Metric.csv')\n",
    "\n",
    "over_all_metrics_stat(skf_metrics, basePath)\n",
    "\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "pd.DataFrame([all_history]).to_csv( basePath+ dbType+'_All_history.csv')\n",
    "\n",
    "Auto_All_History_plot( all_history , basePath ,dbType)\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "# pd.DataFrame([all_UnseenData_results]).to_csv( basePath+ dbType + '_All_UnseenData_Results.csv')\n",
    "\n",
    "df = pd.DataFrame( skf_metrics, columns=['test_accuracy', 'test_precision', 'test_recall' ,'test_f1' , 'test_specificity', 'test_sensitivity', 'test_kappa' ])\n",
    "pd.DataFrame( df ).to_csv( basePath + dbType + '_All_UnseenData_Results_df.csv' , index=False )\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "\n",
    "print(' ok ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# DNC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"DNC memory operations and state.\n",
    "Conventions:\n",
    "    B - batch size\n",
    "    N - number of slots in memory\n",
    "    R - number of read heads\n",
    "\"\"\"\n",
    "\n",
    "from collections import namedtuple\n",
    "import tensorflow as tf\n",
    "\n",
    "EPSILON = 1e-6\n",
    "\n",
    "\n",
    "class ContentAddressing:\n",
    "    \"\"\"\n",
    "    Access memory content using cosine similarity.\n",
    "        Used for: reading, writing\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def weighting(memory_matrix, keys, strengths, sharpness_op=tf.math.softplus):\n",
    "        \"\"\"Get content-based weighting using cosine similarity. The weighting\n",
    "        for each memory slot will be high if the key points in the same\n",
    "        direction as the memory contents at that slot.\n",
    "        Args:\n",
    "            memory_matrix (Tensor [B, N, W]): the memory matrix to query\n",
    "            keys (Tensor [B, W, R]): the keys to query the memory\n",
    "            strengths (Tensor [B, R]): strengths for each lookup key\n",
    "            sharpness_op (fn): operation to transform strengths before softmax\n",
    "        Returns:\n",
    "            Tensor [B, N, R]: lookup weightings for each key\n",
    "        \"\"\"\n",
    "        memory_normalised = tf.math.l2_normalize(memory_matrix, 2, epsilon=EPSILON)\n",
    "        keys_normalised = tf.math.l2_normalize(keys, 1, epsilon=EPSILON)\n",
    "        similiarity = tf.matmul(memory_normalised, keys_normalised)\n",
    "        strengths = tf.expand_dims(sharpness_op(strengths), 1)\n",
    "\n",
    "        return tf.math.softmax(similiarity * strengths, 1)\n",
    "\n",
    "\n",
    "class TemporalLinkAddressing:\n",
    "    \"\"\"\n",
    "    Access memory content by considering which interactions have happened\n",
    "    recently in time.\n",
    "        Used for: reading\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def update_precedence_vector(prev_precedence_vector, write_weighting):\n",
    "        \"\"\"Return next precedence vector by taking into account the writing\n",
    "        action that has just happened via `write_weighting`.\n",
    "        The precedence vector at position `i` denotes the degree to which\n",
    "        memory location `i` has been recently written.\n",
    "        Args:\n",
    "            prev_precedence_vector (Tensor [B, N]): precedence vector from time t-1\n",
    "            write_weighting (Tensor [B, N]): final weighting used to write at time t\n",
    "        Returns:\n",
    "            Tensor [B, N]: precedence vector to use at next time step\n",
    "        \"\"\"\n",
    "        write_strength = tf.reduce_sum(input_tensor=write_weighting, axis=1, keepdims=True)\n",
    "        updated_precedence_vector = (1 - write_strength) * prev_precedence_vector + write_weighting \n",
    "\n",
    "        return updated_precedence_vector\n",
    "\n",
    "    @staticmethod\n",
    "    def update_link_matrix(prev_link_matrix, prev_precedence_vector, write_weighting):\n",
    "        \"\"\"Adjust the link matrix by taking into account the writing\n",
    "        action that has just happened and the previous precedence vector.\n",
    "        Link matrix at `L[t,i,j]` describes the degree to which memory location\n",
    "        `i` was written after location `j` between time `t` and `t+1`.\n",
    "        Args:\n",
    "            prev_link_matrix (Tensor [B, N, N]): link matrix from time t-1\n",
    "            prev_precedence_vector (Tensor [B, N]): precedence vector from time t-1\n",
    "            write_weighting (Tensor [B, N)): final weighting used to write at time t\n",
    "        Returns:\n",
    "            Tensor [B, N, N]: temporal link matrix to use at next time step\n",
    "        \"\"\"\n",
    "        batch_size = prev_link_matrix.shape[0]\n",
    "        words_num = prev_link_matrix.shape[1]\n",
    "\n",
    "        write_weighting_i = tf.expand_dims(write_weighting, 2)  # [b x N x 1 ] duplicate columns\n",
    "        write_weighting_j = tf.expand_dims(write_weighting, 1)  # [b x 1 X N ] duplicate rows\n",
    "        prev_precedence_vector_j = tf.expand_dims(prev_precedence_vector, 1)  # [b x 1 X N]\n",
    "\n",
    "        link_matrix = (\n",
    "            (1 - write_weighting_i - write_weighting_j) * prev_link_matrix\n",
    "            + (write_weighting_i * prev_precedence_vector_j)\n",
    "        )\n",
    "        zero_diagonal = tf.zeros([batch_size, words_num], dtype=link_matrix.dtype)\n",
    "\n",
    "        return tf.linalg.set_diag(link_matrix, zero_diagonal)\n",
    "\n",
    "    @staticmethod\n",
    "    def weightings(link_matrix, prev_read_weightings):\n",
    "        \"\"\"Calculate weightings for each read head so they have a preference\n",
    "        towards directionality.\n",
    "        Args:\n",
    "            link_matrix (Tensor [B, N, N])\n",
    "            prev_read_weightings (Tensor [B, N, R]): read weightings from time t-1\n",
    "        Returns:\n",
    "            Tuple(Tensor [B, N, R], Tensor [B, N, R]): temporal weightings for each memory slot\n",
    "        \"\"\"\n",
    "        forward_weighting = tf.matmul(link_matrix, prev_read_weightings)\n",
    "        backward_weighting = tf.matmul(link_matrix, prev_read_weightings, adjoint_a=True)\n",
    "\n",
    "        return forward_weighting, backward_weighting\n",
    "\n",
    "\n",
    "class AllocationAdressing:\n",
    "    \"\"\"\n",
    "    Access memory content by considering which memory slots can be allocated to.\n",
    "    This is used to provide a differentiable form of dynamic memory allocation\n",
    "    where slots can only be written to if they are determined to be free.\n",
    "        Used for: writing\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def update_usage_vector(free_gates, prev_read_weightings,\n",
    "                            prev_write_weighting, prev_usage_vector):\n",
    "        \"\"\"Adjust the usage vector based on reads and writes from previous time\n",
    "        step.\n",
    "        The usage vector is a helper data structure to aid in the calculation of\n",
    "        the allocation weighting. `u[t,i]` describes the usage between [0,1]\n",
    "        inside memory slot `i` at time `t`. Elements of usage vector may add up\n",
    "        to a maximum of `N`.\n",
    "        The free gate allows reads to happen over multiple time steps at the same\n",
    "        location, otherwise we would always say a location is unused immediately\n",
    "        after a read has occurred.\n",
    "        Args:\n",
    "            free_gates (Tensor [B, R]): current free gate\n",
    "            prev_read_weightings (Tensor [B, N, R]): read weightings from time t-1\n",
    "            prev_write_weighting (Tensor [B, N]): write weighting from time t-1\n",
    "            prev_usage_vector (Tensor [B, N]): usage vector from time t-1\n",
    "        Returns:\n",
    "            Tensor [B, N]: new usage vector\n",
    "        \"\"\"\n",
    "        with tf.name_scope('allocation_addressing'):\n",
    "            retention_vector = tf.reduce_prod(\n",
    "                input_tensor=1 - tf.expand_dims(free_gates, 1) * prev_read_weightings,\n",
    "                axis=2,\n",
    "            )\n",
    "            usage_vector = (\n",
    "                (prev_usage_vector + prev_write_weighting\n",
    "                 - (prev_usage_vector * prev_write_weighting))\n",
    "                * retention_vector\n",
    "            )\n",
    "            return usage_vector\n",
    "\n",
    "    @staticmethod\n",
    "    def batch_unsort(tensor, indices):\n",
    "        \"\"\"Permute each batch in a batch first tensor according to tensor\n",
    "        of indices.\n",
    "        \"\"\"\n",
    "        unpacked = tf.unstack(indices)\n",
    "        indices_inverted = tf.stack(\n",
    "            [tf.math.invert_permutation(permutation) for permutation in unpacked]\n",
    "        )\n",
    "\n",
    "        unpacked = zip(tf.unstack(tensor), tf.unstack(indices_inverted))\n",
    "        return tf.stack([tf.gather(value, index) for value, index in unpacked])\n",
    "\n",
    "    @staticmethod\n",
    "    def weighting(usage_vector):\n",
    "        \"\"\"Calculate allocation weighting so we know which memory slots are\n",
    "        free to be written to. Tells us the degree to which each memory location\n",
    "        is \"allocable\".\n",
    "        Args:\n",
    "            usage_vector (Tensor [B, N]): newly calculated usage vector at time t\n",
    "        Returns:\n",
    "            Tensor [B, N]: allocation weighting for each memory slot\n",
    "        \"\"\"\n",
    "        usage = (1 - EPSILON) * usage_vector + EPSILON\n",
    "        emptiness = 1 - usage\n",
    "\n",
    "        words_num = usage_vector.get_shape().as_list()[1]\n",
    "        emptiness_sorted, free_list = tf.nn.top_k(emptiness, k=words_num)\n",
    "        usage_sorted = 1 - emptiness_sorted\n",
    "        allocation_sorted = emptiness_sorted * tf.math.cumprod(usage_sorted, axis=1, exclusive=True)\n",
    "\n",
    "        return AllocationAdressing.batch_unsort(allocation_sorted, free_list)\n",
    "\n",
    "\n",
    "class Memory:\n",
    "    \"\"\"Differentiable memory for the DNC.\n",
    "    This module implements a recurrent module interface and tracks memory state\n",
    "    through time. Performs a write and read operation given the previous state\n",
    "    and an interface vector defining how to interact with the memory at the\n",
    "    current time step.\n",
    "    Note: although this layer behaves similar to an rnn, it has no parameters\n",
    "    and is actually a deterministic operation:\n",
    "        (interface, prev_memory_state) -> (read_vectors, new_memory_state)\n",
    "    Args:\n",
    "        words_num (int): number of memory slots\n",
    "        word_size (int): size of each memory slot\n",
    "        read_heads_num (int): number of read heads to use inside memory\n",
    "    \"\"\"\n",
    "\n",
    "    state = namedtuple(\n",
    "        \"memory_state\", [\n",
    "            'memory_matrix',\n",
    "            'usage_vector',\n",
    "            'link_matrix',\n",
    "            'precedence_vector',\n",
    "            'write_weighting',\n",
    "            'read_weightings',\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    def __init__(self, words_num=256, word_size=64, read_heads_num=4):\n",
    "        self._N = words_num\n",
    "        self._W = word_size\n",
    "        self._R = read_heads_num\n",
    "\n",
    "    def __call__(self, interface, prev_memory_state):\n",
    "        \"\"\"Define op for the recurrent module.\n",
    "        Args:\n",
    "            interface (namedtuple): parsed interface vector\n",
    "            prev_memory_state (namedtuple): object containing the memory plus all\n",
    "                the helper data structures used to interface with the memory\n",
    "        Returns:\n",
    "            Tuple:\n",
    "                read vectors (Tensor [N, R]): read vectors taken out of the memory\n",
    "                next memory state (namedtuple): new state after write and read\n",
    "        \"\"\"\n",
    "        with tf.name_scope(\"write\"):\n",
    "            usage, write_weighting, memory_matrix, link_matrix, precedence = Memory.write(\n",
    "                prev_memory_state,\n",
    "                interface,\n",
    "            )\n",
    "\n",
    "        with tf.name_scope(\"read\"):\n",
    "            read_weightings, read_vectors = Memory.read(\n",
    "                memory_matrix,\n",
    "                prev_memory_state.read_weightings,\n",
    "                link_matrix,\n",
    "                interface,\n",
    "            )\n",
    "        return read_vectors, Memory.state(\n",
    "            memory_matrix=memory_matrix,\n",
    "            usage_vector=usage,\n",
    "            link_matrix=link_matrix,\n",
    "            precedence_vector=precedence,\n",
    "            write_weighting=write_weighting,\n",
    "            read_weightings=read_weightings,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def read(memory_matrix, prev_read_weightings, link_matrix, interface):\n",
    "        \"\"\"Perform read on memory.\n",
    "        Args:\n",
    "            memory_matrix (Tensor [B, N, W]): memory matrix after recent write at time t\n",
    "            prev_read_weightings (Tensor [B, N, R]): read weightings from time t-1\n",
    "            link_matrix (Tensor [B, N, N]): link matrix after recent write at time t\n",
    "            interface (namedtuple): parsed interface vector\n",
    "        Returns:\n",
    "            Tuple:\n",
    "                read_weightings (Tensor [B, N, R]): read vectors taken out of the memory\n",
    "                read_vectors (Tensor [B, W, R]): read vectors taken out of the memory\n",
    "        \"\"\"\n",
    "        with tf.name_scope(\"content_addressing\"):\n",
    "            lookup_weighting = ContentAddressing.weighting(\n",
    "                memory_matrix,\n",
    "                interface.read_keys,\n",
    "                interface.read_strengths\n",
    "            )\n",
    "        with tf.name_scope(\"temporal_link_addressing\"):\n",
    "            forward_weighting, backward_weighting = TemporalLinkAddressing.weightings(\n",
    "                link_matrix,\n",
    "                prev_read_weightings,\n",
    "            )\n",
    "\n",
    "        with tf.name_scope(\"blend_addressing_modes\"):\n",
    "            read_weightings = tf.einsum(\n",
    "                \"bsr,bnrs->bnr\",\n",
    "                interface.read_modes,\n",
    "                tf.stack([backward_weighting, lookup_weighting, forward_weighting], axis=3)\n",
    "            )\n",
    "        read_vectors = tf.matmul(memory_matrix, read_weightings, adjoint_a=True)\n",
    "\n",
    "        return read_weightings, read_vectors\n",
    "\n",
    "    @staticmethod\n",
    "    def write(prev_memory_state, interface):\n",
    "        \"\"\"Perform write on memory.\n",
    "        Args:\n",
    "            prev_memory_state (namedtuple): memory state from time t-1\n",
    "            interface (namedtuple): parsed interface vector\n",
    "        Returns:\n",
    "            Tuple:\n",
    "                usage_vector (Tensor [B, N])\n",
    "                write_weighting (Tensor [B, N])\n",
    "                memory_matrix (Tensor [B, N, W])\n",
    "                link_matrix (Tensor [B, N, N])\n",
    "                precedence_vector (Tensor [B, N])\n",
    "        \"\"\"\n",
    "        m = prev_memory_state\n",
    "        i = interface\n",
    "\n",
    "        with tf.name_scope(\"calculate_weighting\"):\n",
    "            with tf.name_scope(\"allocation_addressing\"):\n",
    "                usage_vector = AllocationAdressing.update_usage_vector(\n",
    "                    i.free_gates,\n",
    "                    m.read_weightings,\n",
    "                    m.write_weighting,\n",
    "                    m.usage_vector\n",
    "                )\n",
    "                allocation_weighting = AllocationAdressing.weighting(usage_vector)\n",
    "            with tf.name_scope(\"content_addressing\"):\n",
    "                lookup_weighting = ContentAddressing.weighting(\n",
    "                    m.memory_matrix,\n",
    "                    i.write_key,\n",
    "                    i.write_strength\n",
    "                )\n",
    "            write_weighting = (\n",
    "                i.write_gate * (i.allocation_gate * allocation_weighting +\n",
    "                                (1 - i.allocation_gate) * tf.squeeze(lookup_weighting))\n",
    "            )\n",
    "\n",
    "        with tf.name_scope(\"erase_and_write\"):\n",
    "            erase = m.memory_matrix * (\n",
    "                (1 - tf.einsum(\"bn,bw->bnw\", write_weighting, i.erase_vector)))\n",
    "            write = tf.einsum(\"bn,bw->bnw\", write_weighting, i.write_vector)\n",
    "            memory_matrix = erase + write\n",
    "\n",
    "        with tf.name_scope(\"final_update\"):\n",
    "            link_matrix = TemporalLinkAddressing.update_link_matrix(\n",
    "                m.link_matrix,\n",
    "                m.precedence_vector,\n",
    "                write_weighting\n",
    "            )\n",
    "            precedence_vector = TemporalLinkAddressing.update_precedence_vector(\n",
    "                m.precedence_vector,\n",
    "                write_weighting\n",
    "            )\n",
    "\n",
    "        return usage_vector, write_weighting, memory_matrix, \\\n",
    "            link_matrix, precedence_vector\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return Memory.state(\n",
    "            memory_matrix=tf.TensorShape([self._N, self._W]),\n",
    "            usage_vector=tf.TensorShape([self._N]),\n",
    "            link_matrix=tf.TensorShape([self._N, self._N]),\n",
    "            precedence_vector=tf.TensorShape([self._N]),\n",
    "            write_weighting=tf.TensorShape([self._N]),\n",
    "            read_weightings=tf.TensorShape([self._N, self._R]),\n",
    "        )\n",
    "\n",
    "    def get_initial_state(self, batch_size, dtype=tf.float32):\n",
    "        return Memory.state(\n",
    "            memory_matrix=tf.fill([batch_size, self._N, self._W], EPSILON),\n",
    "            usage_vector=tf.zeros([batch_size, self._N], dtype=dtype),\n",
    "            link_matrix=tf.zeros([batch_size, self._N, self._N], dtype=dtype),\n",
    "            precedence_vector=tf.zeros([batch_size, self._N], dtype=dtype),\n",
    "            write_weighting=tf.fill([batch_size, self._N], EPSILON),\n",
    "            read_weightings=tf.fill([batch_size, self._N, self._R], EPSILON),\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# pylint: disable=W0221\n",
    "\"\"\"\n",
    "Differentiable Neural Computer model definition.\n",
    "Reference:\n",
    "    http://www.nature.com/nature/journal/vaop/ncurrent/full/nature20101.html\n",
    "Conventions:\n",
    "    B - batch size\n",
    "    N - number of slots in memory\n",
    "    R - number of read heads\n",
    "    W - size of each memory slot i.e word size\n",
    "\"\"\"\n",
    "\n",
    "from collections import namedtuple, OrderedDict\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.util import nest\n",
    "\n",
    "#from .memory import Memory, EPSILON\n",
    "\n",
    "\n",
    "class DNC(tf.keras.layers.Layer):\n",
    "    \"\"\"DNC recurrent module that connects together the controller and memory.\n",
    "    Performs a write and read operation against memory given 1) the previous state\n",
    "    and 2) an interface vector defining how to interact with the memory at the\n",
    "    current time step.\n",
    "    Args:\n",
    "        output_size (int): size of final output dimension for the whole DNC cell at each time step\n",
    "        controller_units (int): size of hidden state in controller\n",
    "        memory_size (int): number of slots in external memory\n",
    "        word_size (int): the width of each memory slot\n",
    "        num_read_heads (int): number of memory read heads\n",
    "    \"\"\"\n",
    "\n",
    "    state = namedtuple(\"dnc_state\", [\n",
    "        \"memory_state\",\n",
    "        \"controller_state\",\n",
    "        \"read_vectors\",\n",
    "    ])\n",
    "\n",
    "    interface = namedtuple(\"interface\", [\n",
    "        \"read_keys\",\n",
    "        \"read_strengths\",\n",
    "        \"write_key\",\n",
    "        \"write_strength\",\n",
    "        \"erase_vector\",\n",
    "        \"write_vector\",\n",
    "        \"free_gates\",\n",
    "        \"allocation_gate\",\n",
    "        \"write_gate\",\n",
    "        \"read_modes\",\n",
    "    ])\n",
    "\n",
    "    def __init__(self, output_size, controller_units=256, memory_size=256,\n",
    "                 word_size=64, num_read_heads=4, **kwargs):\n",
    "        super().__init__(name='DNC', **kwargs)\n",
    "\n",
    "        self._output_size = output_size\n",
    "        self._N = memory_size\n",
    "        self._R = num_read_heads\n",
    "        self._W = word_size\n",
    "        self._interface_vector_size = self._R * self._W + 3 * self._W + 5 * self._R + 3\n",
    "        self._clip = 20.0\n",
    "        \n",
    "      \n",
    "        self._controller = tf.keras.layers.LSTMCell(units=controller_units)\n",
    "        self._controller_to_interface_dense = tf.keras.layers.Dense(\n",
    "            self._interface_vector_size,\n",
    "            name='controller_to_interface'\n",
    "        )\n",
    "        #--------------------------------------------------------\n",
    "        \n",
    "\n",
    "        # self._controller_to_interface_LSTM = tf.keras.layers.LSTM(\n",
    "        #     units=16,\n",
    "        #     #return_sequences=True,\n",
    "        #     return_state=True,\n",
    "        #     name='controller_to_interface_LSTM'\n",
    "        # )\n",
    "\n",
    "        # self._controller_to_interface_TimeDistributed = tf.keras.layers.TimeDistributed(\n",
    "        #     tf.keras.layers.Dense( 256 ),\n",
    "        #     name='controller_to_interface_TimeDistributed'\n",
    "        # )\n",
    "        self._controller_to_interface_dense1 = tf.keras.layers.Dense(\n",
    "            256,\n",
    "            name='controller_to_interface_Dense1'\n",
    "        )\n",
    "        self._controller_to_interface_dense2 = tf.keras.layers.Dense(\n",
    "            16,\n",
    "            name='controller_to_interface_Dense2'\n",
    "        )\n",
    "        self._controller_to_interface_Dropout = tf.keras.layers.Dropout(\n",
    "            0.1,\n",
    "            name='controller_to_interface_Dropout'\n",
    "        )\n",
    "        self._controller_to_interface_Flatten = tf.keras.layers.Flatten(           \n",
    "            name='controller_to_interface_Flatten'\n",
    "        )        \n",
    "        #--------------------------------------------------------\n",
    "        self._memory = Memory(memory_size, word_size, num_read_heads)\n",
    "        self._final_output_dense = tf.keras.layers.Dense(self._output_size)\n",
    "\n",
    "    def _parse_interface_vector(self, interface_vector):\n",
    "        r, w = self._R, self._W\n",
    "\n",
    "        sizes = [r * w, r, w, 1, w, w, r, 1, 1, 3 * r]\n",
    "        fns = OrderedDict([\n",
    "            (\"read_keys\", lambda v: tf.reshape(v, (-1, w, r))),\n",
    "            (\"read_strengths\", lambda v: 1 + tf.nn.softplus((tf.reshape(v, (-1, r))))),\n",
    "            (\"write_key\", lambda v: tf.reshape(v, (-1, w, 1))),\n",
    "            (\"write_strength\", lambda v: 1 + tf.nn.softplus((tf.reshape(v, (-1, 1))))),\n",
    "            (\"erase_vector\", lambda v: tf.nn.sigmoid(tf.reshape(v, (-1, w)))),\n",
    "            (\"write_vector\", lambda v: tf.reshape(v, (-1, w))),\n",
    "            (\"free_gates\", lambda v: tf.nn.sigmoid(tf.reshape(v, (-1, r)))),\n",
    "            (\"allocation_gate\", lambda v: tf.nn.sigmoid(tf.reshape(v, (-1, 1)))),\n",
    "            (\"write_gate\", lambda v: tf.nn.sigmoid(tf.reshape(v, (-1, 1)))),\n",
    "            (\"read_modes\", lambda v: tf.nn.softmax(tf.reshape(v, (-1, 3, r)), axis=1)),\n",
    "        ])\n",
    "        indices = [[sum(sizes[:i]), sum(sizes[:i + 1])] for i in range(len(sizes))]\n",
    "        zipped_items = zip(fns.keys(), fns.values(), indices)\n",
    "        interface = {name: fn(interface_vector[:, i[0]:i[1]]) for name, fn, i in zipped_items}\n",
    "\n",
    "        return DNC.interface(**interface)\n",
    "\n",
    "    def _flatten_read_vectors(self, x):\n",
    "        return tf.reshape(x, (-1, self._W * self._R))\n",
    "\n",
    "    def call(self, inputs, prev_dnc_state):\n",
    "        prev_dnc_state = nest.pack_sequence_as(self.state_size_nested, prev_dnc_state)\n",
    "        with tf.name_scope(\"inputs_to_controller\"):\n",
    "            read_vectors_flat = self._flatten_read_vectors(prev_dnc_state.read_vectors)\n",
    "            input_augmented = tf.concat([inputs, read_vectors_flat], 1)\n",
    "\n",
    "           \n",
    "            controller_output, controller_state = self._controller(\n",
    "                input_augmented,\n",
    "                prev_dnc_state.controller_state,               \n",
    "            )\n",
    "\n",
    "            \n",
    "\n",
    "            # # # ------------------------------------------------            \n",
    "            #  lstm1, state_h, state_c\n",
    "            #controller_output44 ,state_h, controller_state   = self._controller_to_interface_LSTM( prev_dnc_state.read_vectors )# karimian TD           \n",
    "            #final_output =self._controller_to_interface_TimeDistributed( final_output)# karimian TD                       \n",
    "            \n",
    "            controller_output =self._controller_to_interface_dense1(controller_output)# karimian TD\n",
    "            #controller_output =self._controller_to_interface_dense2(controller_output)# karimian TD\n",
    "            #controller_output =self._controller_to_interface_Dropout(controller_output)# karimian TD\n",
    "            #controller_output =self._controller_to_interface_Flatten(controller_output)# karimian TD            \n",
    "            # ------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "            controller_output = tf.clip_by_value( controller_output, -self._clip, self._clip)\n",
    "\n",
    "        with tf.name_scope(\"parse_interface\"):          \n",
    "            #controller_output =self._controller_to_interface_dense2(controller_output)# karimian TD\n",
    "            interface = self._controller_to_interface_dense( controller_output)           \n",
    "            interface =self._controller_to_interface_Dropout(interface)# karimian TD\n",
    "            interface =self._controller_to_interface_Flatten(interface)# karimian TD \n",
    "            \n",
    "            interface = self._parse_interface_vector(interface)\n",
    "\n",
    "        with tf.name_scope(\"update_memory\"):\n",
    "            read_vectors, memory_state = self._memory(interface, prev_dnc_state.memory_state)           \n",
    "            state = DNC.state(\n",
    "                memory_state=memory_state,\n",
    "                controller_state= controller_state,\n",
    "                read_vectors=read_vectors,\n",
    "            )\n",
    "\n",
    "        with tf.name_scope(\"join_outputs\"):\n",
    "            read_vectors_flat = self._flatten_read_vectors(read_vectors)\n",
    "            final_output = tf.concat([controller_output, read_vectors_flat], 1)\n",
    "            \n",
    "\n",
    "         \n",
    "            # # ------------------------------------------------            \n",
    "            # final_output = self._controller_to_interface_LSTM(   read_vectors        )            \n",
    "            # final_output =self._controller_to_interface_TimeDistributed( final_output)                       \n",
    "            \n",
    "            # final_output =self._controller_to_interface_dense2(final_output)\n",
    "            # final_output =self._controller_to_interface_Dropout(final_output)\n",
    "            # final_output =self._controller_to_interface_Flatten(final_output)            \n",
    "            # # ------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            final_output = self._final_output_dense(final_output)\n",
    "            final_output = tf.clip_by_value(final_output, -self._clip, self._clip)\n",
    "\n",
    "        return final_output, nest.flatten(state)\n",
    "\n",
    "    @property\n",
    "    def state_size_nested(self):\n",
    "        return DNC.state(\n",
    "            memory_state=self._memory.state_size,\n",
    "            controller_state=self._controller.state_size,\n",
    "            read_vectors=tf.TensorShape([self._W, self._R]),\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return nest.flatten(self.state_size_nested)\n",
    "\n",
    "    def get_initial_state(self, inputs=None, batch_size=None, dtype=tf.float32):\n",
    "        del inputs\n",
    "        initial_state_nested = DNC.state(\n",
    "            memory_state=self._memory.get_initial_state(batch_size, dtype=dtype),\n",
    "            controller_state=self._controller.get_initial_state(batch_size=batch_size, dtype=dtype),\n",
    "            read_vectors=tf.fill([batch_size, self._W, self._R], EPSILON),\n",
    "        )\n",
    "        return nest.flatten(initial_state_nested)\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._output_size\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'controller': {\n",
    "                'class_name': self._controller.__class__.__name__,\n",
    "                'config': self._controller.get_config()\n",
    "            },\n",
    "            'memory': {\n",
    "                'read_heads_num': self._R,\n",
    "                'word_size': self._W,\n",
    "                'words_num': self._N,\n",
    "            },\n",
    "            'clip': self._clip,\n",
    "            'output_size': self._output_size,\n",
    "        }\n",
    "        base_config = super().get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "1EODUBQj-zPU",
    "h11kVYItZ-qn",
    "aTb_J9rhuBs6",
    "JiZUQk7CYqN0",
    "Fjo7iI5D3Rt8",
    "igSfgMLqu2Or",
    "he6ydZETuYlR",
    "SE44B-tSXD9d",
    "J-hSY5oGoC2s",
    "fz35La2wB_E_",
    "U-m8GMo1CToC",
    "qdRHc9ewBXdn",
    "zl31eNvfBbD1",
    "Oi-Bnq7kKj6B"
   ],
   "name": "TDNTM_IV2A_Imagery_14000720.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
